groups:
- name: node-exporter.rules
  rules:
  - expr: rate(node_vmstat_pgmajfault{job="kubernetes-service-endpoints"}[5m])
    record: instance:node_vmstat_pgmajfault:rate5m
  - expr: rate(node_disk_io_time_seconds_total{job="kubernetes-service-endpoints",
      device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
    record: instance_device:node_disk_io_time_seconds:rate5m
  - expr: rate(node_disk_io_time_weighted_seconds_total{job="kubernetes-service-endpoints",
      device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
    record: instance_device:node_disk_io_time_weighted_seconds:rate5m
- name: kube-apiserver-slos
  rules:
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget on cluster {{ $labels.cluster
        }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: 'sum by (cluster) (apiserver_request:burnrate1h) > (14.40 * 0.01000)

      and on (cluster)

      sum by (cluster) (apiserver_request:burnrate5m) > (14.40 * 0.01000)'
    for: 2m
    labels:
      long: 1h
      severity: critical
      short: 5m
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget on cluster {{ $labels.cluster
        }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: 'sum by (cluster) (apiserver_request:burnrate6h) > (6.00 * 0.01000)

      and on (cluster)

      sum by (cluster) (apiserver_request:burnrate30m) > (6.00 * 0.01000)'
    for: 15m
    labels:
      long: 6h
      severity: critical
      short: 30m
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget on cluster {{ $labels.cluster
        }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: 'sum by (cluster) (apiserver_request:burnrate1d) > (3.00 * 0.01000)

      and on (cluster)

      sum by (cluster) (apiserver_request:burnrate2h) > (3.00 * 0.01000)'
    for: 1h
    labels:
      long: 1d
      severity: warning
      short: 2h
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget on cluster {{ $labels.cluster
        }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: 'sum by (cluster) (apiserver_request:burnrate3d) > (1.00 * 0.01000)

      and on (cluster)

      sum by (cluster) (apiserver_request:burnrate6h) > (1.00 * 0.01000)'
    for: 3h
    labels:
      long: 3d
      severity: warning
      short: 6h
- name: node-network
  rules:
  - alert: NodeNetworkInterfaceFlapping
    annotations:
      description: Network interface "{{ $labels.device }}" changing its up status
        often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping
      summary: Network interface is often changing its status
    expr: changes(node_network_up{job="kubernetes-service-endpoints",device!~"veth.+"}[2m])
      > 2
    for: 2m
    labels:
      severity: warning
- name: kubernetes-system-kubelet
  rules:
  - alert: KubeNodeNotReady
    annotations:
      description: '{{ $labels.node }} has been unready for more than 15 minutes on
        cluster {{ $labels.cluster }}.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
      summary: Node is not ready.
    expr: 'kube_node_status_condition{job="kubernetes-service-endpoints",condition="Ready",status="true"}
      == 0

      and on (cluster, node)

      kube_node_spec_unschedulable{job="kubernetes-service-endpoints"} == 0'
    for: 15m
    labels:
      severity: warning
  - alert: KubeNodePressure
    annotations:
      description: '{{ $labels.node }} on cluster {{ $labels.cluster }} has active
        Condition {{ $labels.condition }}. This is caused by resource usage exceeding
        eviction thresholds.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodepressure
      summary: Node has as active Condition.
    expr: 'kube_node_status_condition{job="kubernetes-service-endpoints",condition=~"(MemoryPressure|DiskPressure|PIDPressure)",status="true"}
      == 1

      and on (cluster, node)

      kube_node_spec_unschedulable{job="kubernetes-service-endpoints"} == 0'
    for: 10m
    labels:
      severity: info
  - alert: KubeNodeUnreachable
    annotations:
      description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled
        on cluster {{ $labels.cluster }}.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
      summary: Node is unreachable.
    expr: (kube_node_spec_taint{job="kubernetes-service-endpoints",key="node.kubernetes.io/unreachable",effect="NoSchedule"}
      unless ignoring(key,value) kube_node_spec_taint{job="kubernetes-service-endpoints",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"})
      == 1
    for: 15m
    labels:
      severity: warning
  - alert: KubeNodeReadinessFlapping
    annotations:
      description: The readiness status of node {{ $labels.node }} has changed {{
        $value }} times in the last 15 minutes on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping
      summary: Node readiness status is flapping.
    expr: 'sum(changes(kube_node_status_condition{job="kubernetes-service-endpoints",status="true",condition="Ready"}[15m]))
      by (cluster, node) > 2

      and on (cluster, node)

      kube_node_spec_unschedulable{job="kubernetes-service-endpoints"} == 0'
    for: 15m
    labels:
      severity: warning
  - alert: KubeletPlegDurationHigh
    annotations:
      description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile
        duration of {{ $value }} seconds on node {{ $labels.node }} on cluster {{
        $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh
      summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
    expr: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"}
      >= 10
    for: 5m
    labels:
      severity: warning
  - alert: KubeletPodStartUpLatencyHigh
    annotations:
      description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds
        on node {{ $labels.node }} on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh
      summary: Kubelet Pod startup latency is too high.
    expr: histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubernetes-nodes-cadvisor",
      metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on (cluster, instance)
      group_left(node) kubelet_node_name{job="kubernetes-nodes-cadvisor", metrics_path="/metrics"}
      > 60
    for: 15m
    labels:
      severity: warning
  - alert: KubeletClientCertificateExpiration
    annotations:
      description: Client certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
      summary: Kubelet client certificate is about to expire.
    expr: kubelet_certificate_manager_client_ttl_seconds < 604800
    labels:
      severity: warning
  - alert: KubeletClientCertificateExpiration
    annotations:
      description: Client certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
      summary: Kubelet client certificate is about to expire.
    expr: kubelet_certificate_manager_client_ttl_seconds < 86400
    labels:
      severity: critical
  - alert: KubeletServerCertificateExpiration
    annotations:
      description: Server certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
      summary: Kubelet server certificate is about to expire.
    expr: kubelet_certificate_manager_server_ttl_seconds < 604800
    labels:
      severity: warning
  - alert: KubeletServerCertificateExpiration
    annotations:
      description: Server certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
      summary: Kubelet server certificate is about to expire.
    expr: kubelet_certificate_manager_server_ttl_seconds < 86400
    labels:
      severity: critical
  - alert: KubeletClientCertificateRenewalErrors
    annotations:
      description: Kubelet on node {{ $labels.node }} has failed to renew its client
        certificate ({{ $value | humanize }} errors in the last 5 minutes) on cluster
        {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors
      summary: Kubelet has failed to renew its client certificate.
    expr: increase(kubelet_certificate_manager_client_expiration_renew_errors[5m])
      > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeletServerCertificateRenewalErrors
    annotations:
      description: Kubelet on node {{ $labels.node }} has failed to renew its server
        certificate ({{ $value | humanize }} errors in the last 5 minutes) on cluster
        {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors
      summary: Kubelet has failed to renew its server certificate.
    expr: increase(kubelet_server_expiration_renew_errors[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeletDown
    annotations:
      description: Kubelet has disappeared from Prometheus target discovery.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
      summary: Target disappeared from Prometheus target discovery.
    expr: absent(up{job="kubernetes-nodes-cadvisor", metrics_path="/metrics"} == 1)
    for: 15m
    labels:
      severity: critical
- name: kube-apiserver-histogram.rules
  rules:
  - expr: histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_sli_duration_seconds_bucket{job="kubernetes-apiservers",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m])))
      > 0
    labels:
      quantile: '0.99'
      verb: read
    record: cluster_quantile:apiserver_request_sli_duration_seconds:histogram_quantile
  - expr: histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_sli_duration_seconds_bucket{job="kubernetes-apiservers",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m])))
      > 0
    labels:
      quantile: '0.99'
      verb: write
    record: cluster_quantile:apiserver_request_sli_duration_seconds:histogram_quantile
- name: kube-prometheus-node-recording.rules
  rules:
  - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m]))
      BY (instance)
    record: instance:node_cpu:rate:sum
  - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
    record: instance:node_network_receive_bytes:rate:sum
  - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
    record: instance:node_network_transmit_bytes:rate:sum
  - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
      WITHOUT (cpu, mode) / ON (instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
      BY (instance, cpu)) BY (instance)
    record: instance:node_cpu:ratio
  - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
    record: cluster:node_cpu:sum_rate5m
  - expr: cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance,
      cpu))
    record: cluster:node_cpu:ratio
- name: kubernetes-apps
  rules:
  - alert: KubePodCrashLooping
    annotations:
      description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
        }}) is in waiting state (reason: "CrashLoopBackOff") on cluster {{ $labels.cluster
        }}.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
      summary: Pod is crash looping.
    expr: max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff",
      job="kubernetes-service-endpoints", namespace=~".*"}[5m]) >= 1
    for: 15m
    labels:
      severity: warning
  - alert: KubeDeploymentRolloutStuck
    annotations:
      description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment
        }} is not progressing for longer than 15 minutes on cluster {{ $labels.cluster
        }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentrolloutstuck
      summary: Deployment rollout is not progressing.
    expr: 'kube_deployment_status_condition{condition="Progressing", status="false",job="kubernetes-service-endpoints",
      namespace=~".*"}

      != 0'
    for: 15m
    labels:
      severity: warning
  - alert: KubeContainerWaiting
    annotations:
      description: 'pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on
        container {{ $labels.container}} has been in waiting state for longer than
        1 hour. (reason: "{{ $labels.reason }}") on cluster {{ $labels.cluster }}.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
      summary: Pod container waiting longer than 1 hour
    expr: kube_pod_container_status_waiting_reason{reason!="CrashLoopBackOff", job="kubernetes-service-endpoints",
      namespace=~".*"} > 0
    for: 1h
    labels:
      severity: warning
  - alert: KubeDaemonSetMisScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are running where they are not supposed to run on cluster {{ $labels.cluster
        }}.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
      summary: DaemonSet pods are misscheduled.
    expr: kube_daemonset_status_number_misscheduled{job="kubernetes-service-endpoints",
      namespace=~".*"} > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeJobFailed
    annotations:
      description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        Removing failed job after investigation should clear this alert on cluster
        {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
      summary: Job failed to complete.
    expr: kube_job_failed{job="kubernetes-service-endpoints", namespace=~".*"}  >
      0
    for: 15m
    labels:
      severity: warning
- name: kube-prometheus-general.rules
  rules:
  - expr: count without(instance, pod, node) (up == 1)
    record: count:up1
  - expr: count without(instance, pod, node) (up == 0)
    record: count:up0
- name: kubernetes-system-apiserver
  rules:
  - alert: KubeClientCertificateExpiration
    annotations:
      description: A client certificate used to authenticate to kubernetes apiserver
        is expiring in less than 7.0 days on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
      summary: Client certificate is about to expire.
    expr: 'histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m])))
      < 604800

      and

      on (job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"}
      > 0'
    for: 5m
    labels:
      severity: warning
  - alert: KubeClientCertificateExpiration
    annotations:
      description: A client certificate used to authenticate to kubernetes apiserver
        is expiring in less than 24.0 hours on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
      summary: Client certificate is about to expire.
    expr: 'histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m])))
      < 86400

      and

      on (job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"}
      > 0'
    for: 5m
    labels:
      severity: critical
  - alert: KubeAggregatedAPIErrors
    annotations:
      description: Kubernetes aggregated API {{ $labels.instance }}/{{ $labels.name
        }} has reported {{ $labels.reason }} errors on cluster {{ $labels.cluster
        }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors
      summary: Kubernetes aggregated API has reported errors.
    expr: sum by (cluster, instance, name, reason)(increase(aggregator_unavailable_apiservice_total{job="kubernetes-apiservers"}[1m]))
      > 0
    for: 10m
    labels:
      severity: warning
  - alert: KubeAggregatedAPIDown
    annotations:
      description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace
        }} has been only {{ $value | humanize }}% available over the last 10m on cluster
        {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown
      summary: Kubernetes aggregated API is down.
    expr: (1 - max by (name, namespace, cluster)(avg_over_time(aggregator_unavailable_apiservice{job="kubernetes-apiservers"}[10m])))
      * 100 < 85
    for: 5m
    labels:
      severity: warning
  - alert: KubeAPIDown
    annotations:
      description: KubeAPI has disappeared from Prometheus target discovery.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown
      summary: Target disappeared from Prometheus target discovery.
    expr: absent(up{job="kubernetes-apiservers"} == 1)
    for: 15m
    labels:
      severity: critical
  - alert: KubeAPITerminatedRequests
    annotations:
      description: The kubernetes apiserver has terminated {{ $value | humanizePercentage
        }} of its incoming requests on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests
      summary: The kubernetes apiserver has terminated {{ $value | humanizePercentage
        }} of its incoming requests.
    expr: sum by (cluster) (rate(apiserver_request_terminations_total{job="kubernetes-apiservers"}[10m]))
      / ( sum by (cluster) (rate(apiserver_request_total{job="kubernetes-apiservers"}[10m]))
      + sum by (cluster) (rate(apiserver_request_terminations_total{job="kubernetes-apiservers"}[10m]))
      ) > 0.20
    for: 5m
    labels:
      severity: warning
- name: kube-state-metrics
  rules:
  - alert: KubeStateMetricsShardingMismatch
    annotations:
      description: kube-state-metrics pods are running with different --total-shards
        configuration, some Kubernetes objects may be exposed multiple times or not
        exposed at all.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch
      summary: kube-state-metrics sharding is misconfigured.
    expr: stdvar (kube_state_metrics_total_shards{job="kubernetes-service-endpoints"})
      by (cluster) != 0
    for: 15m
    labels:
      severity: critical
- name: kubelet.rules
  rules:
  - expr: histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubernetes-nodes-cadvisor",
      metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on (cluster, instance)
      group_left(node) kubelet_node_name{job="kubernetes-nodes-cadvisor", metrics_path="/metrics"})
    labels:
      quantile: '0.99'
    record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
  - expr: histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubernetes-nodes-cadvisor",
      metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on (cluster, instance)
      group_left(node) kubelet_node_name{job="kubernetes-nodes-cadvisor", metrics_path="/metrics"})
    labels:
      quantile: '0.9'
    record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
  - expr: histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubernetes-nodes-cadvisor",
      metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on (cluster, instance)
      group_left(node) kubelet_node_name{job="kubernetes-nodes-cadvisor", metrics_path="/metrics"})
    labels:
      quantile: '0.5'
    record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
- name: node-exporter
  rules:
  - alert: NodeNetworkReceiveErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf "%.0f" $value }} receive errors in the last two minutes.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs
      summary: Network interface is reporting many receive errors.
    expr: rate(node_network_receive_errs_total{job="kubernetes-service-endpoints"}[2m])
      / rate(node_network_receive_packets_total{job="kubernetes-service-endpoints"}[2m])
      > 0.01
    for: 1h
    labels:
      severity: warning
  - alert: NodeNetworkTransmitErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs
      summary: Network interface is reporting many transmit errors.
    expr: rate(node_network_transmit_errs_total{job="kubernetes-service-endpoints"}[2m])
      / rate(node_network_transmit_packets_total{job="kubernetes-service-endpoints"}[2m])
      > 0.01
    for: 1h
    labels:
      severity: warning
  - alert: NodeHighNumberConntrackEntriesUsed
    annotations:
      description: '{{ $labels.instance }} {{ $value | humanizePercentage }} of conntrack
        entries are used.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused
      summary: Number of conntrack are getting close to the limit.
    expr: (node_nf_conntrack_entries{job="kubernetes-service-endpoints"} / node_nf_conntrack_entries_limit)
      > 0.75
    labels:
      severity: warning
  - alert: NodeTextFileCollectorScrapeError
    annotations:
      description: Node Exporter text file collector on {{ $labels.instance }} failed
        to scrape.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror
      summary: Node Exporter text file collector failed to scrape.
    expr: node_textfile_scrape_error{job="kubernetes-service-endpoints"} == 1
    labels:
      severity: warning
  - alert: NodeClockNotSynchronising
    annotations:
      description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP
        is configured on this host.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising
      summary: Clock not synchronising.
    expr: 'min_over_time(node_timex_sync_status{job="kubernetes-service-endpoints"}[5m])
      == 0

      and

      node_timex_maxerror_seconds{job="kubernetes-service-endpoints"} >= 16'
    for: 10m
    labels:
      severity: warning
  - alert: NodeRAIDDegraded
    annotations:
      description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is
        in degraded state due to one or more disks failures. Number of spare drives
        is insufficient to fix issue automatically.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded
      summary: RAID Array is degraded.
    expr: node_md_disks_required{job="kubernetes-service-endpoints",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}
      - ignoring (state) (node_md_disks{state="active",job="kubernetes-service-endpoints",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"})
      > 0
    for: 15m
    labels:
      severity: critical
  - alert: NodeRAIDDiskFailure
    annotations:
      description: At least one device in RAID array at {{ $labels.instance }} failed.
        Array '{{ $labels.device }}' needs attention and possibly a disk swap.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure
      summary: Failed device in RAID array.
    expr: node_md_disks{state="failed",job="kubernetes-service-endpoints",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}
      > 0
    labels:
      severity: warning
  - alert: NodeCPUHighUsage
    annotations:
      description: 'CPU usage at {{ $labels.instance }} has been above 90% for the
        last 15 minutes, is currently at {{ printf "%.2f" $value }}%.

        '
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodecpuhighusage
      summary: High CPU usage.
    expr: sum without(mode) (avg without (cpu) (rate(node_cpu_seconds_total{job="kubernetes-service-endpoints",
      mode!~"idle|iowait"}[2m]))) * 100 > 90
    for: 15m
    labels:
      severity: info
  - alert: NodeSystemSaturation
    annotations:
      description: 'System load per core at {{ $labels.instance }} has been above
        2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.

        This might indicate this instance resources saturation and can cause it becoming
        unresponsive.

        '
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation
      summary: System saturated, load per core is very high.
    expr: 'node_load1{job="kubernetes-service-endpoints"}

      / count without (cpu, mode) (node_cpu_seconds_total{job="kubernetes-service-endpoints",
      mode="idle"}) > 2'
    for: 15m
    labels:
      severity: warning
  - alert: NodeMemoryMajorPagesFaults
    annotations:
      description: 'Memory major pages are occurring at very high rate at {{ $labels.instance
        }}, 500 major page faults per second for the last 15 minutes, is currently
        at {{ printf "%.2f" $value }}.

        Please check that there is enough memory available at this instance.

        '
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememorymajorpagesfaults
      summary: Memory major page faults are occurring at very high rate.
    expr: rate(node_vmstat_pgmajfault{job="kubernetes-service-endpoints"}[5m]) > 500
    for: 15m
    labels:
      severity: warning
  - alert: NodeMemoryHighUtilization
    annotations:
      description: 'Memory is filling up at {{ $labels.instance }}, has been above
        90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.

        '
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememoryhighutilization
      summary: Host is running out of memory.
    expr: 100 - (node_memory_MemAvailable_bytes{job="kubernetes-service-endpoints"}
      / node_memory_MemTotal_bytes{job="kubernetes-service-endpoints"} * 100) > 90
    for: 15m
    labels:
      severity: warning
  - alert: NodeDiskIOSaturation
    annotations:
      description: 'Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance
        }}, has been above 10 for the last 30 minutes, is currently at {{ printf "%.2f"
        $value }}.

        This symptom might indicate disk saturation.

        '
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation
      summary: Disk IO queue is high.
    expr: rate(node_disk_io_time_weighted_seconds_total{job="kubernetes-service-endpoints",
      device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
      > 10
    for: 30m
    labels:
      severity: warning
  - alert: NodeSystemdServiceFailed
    annotations:
      description: Systemd service {{ $labels.name }} has entered failed state at
        {{ $labels.instance }}
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicefailed
      summary: Systemd service has entered failed state.
    expr: node_systemd_unit_state{job="kubernetes-service-endpoints", state="failed"}
      == 1
    for: 5m
    labels:
      severity: warning
  - alert: NodeSystemdServiceCrashlooping
    annotations:
      description: Systemd service {{ $labels.name }} has being restarted too many
        times at {{ $labels.instance }} for the last 15 minutes. Please check if service
        is crash looping.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicecrashlooping
      summary: Systemd service keeps restaring, possibly crash looping.
    expr: increase(node_systemd_service_restart_total{job="kubernetes-service-endpoints"}[5m])
      > 2
    for: 15m
    labels:
      severity: warning
  - alert: NodeBondingDegraded
    annotations:
      description: Bonding interface {{ $labels.master }} on {{ $labels.instance }}
        is in degraded state due to one or more slave failures.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodebondingdegraded
      summary: Bonding interface is degraded
    expr: (node_bonding_slaves - node_bonding_active) != 0
    for: 5m
    labels:
      severity: warning
- name: general.rules
  rules:
  - alert: TargetDown
    annotations:
      description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
        }} targets in {{ $labels.namespace }} namespace are down.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/targetdown
      summary: One or more targets are unreachable.
    expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,
      namespace, service)) > 10
    for: 10m
    labels:
      severity: warning
  - alert: Watchdog
    annotations:
      description: 'This is an alert meant to ensure that the entire alerting pipeline
        is functional.

        This alert is always firing, therefore it should always be firing in Alertmanager

        and always fire against a receiver. There are integrations with various notification

        mechanisms that send a notification when this alert is not firing. For example
        the

        "DeadMansSnitch" integration in PagerDuty.

        '
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/watchdog
      summary: An alert that should always be firing to certify that Alertmanager
        is working properly.
    expr: vector(1)
    labels:
      severity: none
  - alert: InfoInhibitor
    annotations:
      description: 'This is an alert that is used to inhibit info alerts.

        By themselves, the info-level alerts are sometimes very noisy, but they are
        relevant when combined with

        other alerts.

        This alert fires whenever there''s a severity="info" alert, and stops firing
        when another alert with a

        severity of ''warning'' or ''critical'' starts firing on the same namespace.

        This alert should be routed to a null receiver and configured to inhibit alerts
        with severity="info".

        '
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor
      summary: Info-level alert inhibition.
    expr: ALERTS{severity = "info"} == 1 unless on (namespace) ALERTS{alertname !=
      "InfoInhibitor", severity =~ "warning|critical", alertstate="firing"} == 1
    labels:
      severity: none
- name: kubernetes-system
  rules:
  - alert: KubeVersionMismatch
    annotations:
      description: There are {{ $value }} different semantic versions of Kubernetes
        components running on cluster {{ $labels.cluster }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch
      summary: Different semantic versions of Kubernetes components running.
    expr: count by (cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"git_version","$1","git_version","(v[0-9]*.[0-9]*).*")))
      > 1
    for: 15m
    labels:
      severity: warning
- name: kubernetes-resources
  rules:
  - alert: KubeCPUOvercommit
    annotations:
      description: Cluster {{ $labels.cluster }} has overcommitted CPU resource requests
        for Pods by {{ printf "%.2f" $value }} CPU shares and cannot tolerate node
        failure.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
      summary: Cluster has overcommitted CPU resource requests.
    expr: 'sum(namespace_cpu:kube_pod_container_resource_requests:sum{}) by (cluster)
      - (sum(kube_node_status_allocatable{job="kubernetes-service-endpoints",resource="cpu"})
      by (cluster) - max(kube_node_status_allocatable{job="kubernetes-service-endpoints",resource="cpu"})
      by (cluster)) > 0

      and

      (sum(kube_node_status_allocatable{job="kubernetes-service-endpoints",resource="cpu"})
      by (cluster) - max(kube_node_status_allocatable{job="kubernetes-service-endpoints",resource="cpu"})
      by (cluster)) > 0'
    for: 10m
    labels:
      severity: warning
  - alert: KubeMemoryOvercommit
    annotations:
      description: Cluster {{ $labels.cluster }} has overcommitted memory resource
        requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node
        failure.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
      summary: Cluster has overcommitted memory resource requests.
    expr: 'sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster)
      - (sum(kube_node_status_allocatable{resource="memory", job="kubernetes-service-endpoints"})
      by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kubernetes-service-endpoints"})
      by (cluster)) > 0

      and

      (sum(kube_node_status_allocatable{resource="memory", job="kubernetes-service-endpoints"})
      by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kubernetes-service-endpoints"})
      by (cluster)) > 0'
    for: 10m
    labels:
      severity: warning
- name: etcd
  rules:
  - alert: etcdMembersDown
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": members are down ({{ $value
        }}).'
      summary: etcd cluster members are down.
    expr: "max without (endpoint) (\n  sum without (instance, pod) (up{job=~\".*etcd.*\"\
      } == bool 0)\nor\n  count without (To) (\n    sum without (instance, pod) (rate(etcd_network_peer_sent_failures_total{job=~\"\
      .*etcd.*\"}[120s])) > 0.01\n  )\n)\n> 0"
    for: 20m
    labels:
      severity: warning
  - alert: etcdInsufficientMembers
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": insufficient members ({{ $value
        }}).'
      summary: etcd cluster has insufficient number of members.
    expr: sum(up{job=~".*etcd.*"} == bool 1) without (instance, pod) < ((count(up{job=~".*etcd.*"})
      without (instance, pod) + 1) / 2)
    for: 3m
    labels:
      severity: critical
  - alert: etcdNoLeader
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }}
        has no leader.'
      summary: etcd cluster has no leader.
    expr: etcd_server_has_leader{job=~".*etcd.*"} == 0
    for: 1m
    labels:
      severity: critical
  - alert: etcdHighNumberOfLeaderChanges
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": {{ $value }} leader changes
        within the last 15 minutes. Frequent elections may be a sign of insufficient
        resources, high network latency, or disruptions by other components and should
        be investigated.'
      summary: etcd cluster has high number of leader changes.
    expr: increase((max without (instance, pod) (etcd_server_leader_changes_seen_total{job=~".*etcd.*"})
      or 0*absent(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}))[15m:1m])
      >= 4
    for: 5m
    labels:
      severity: warning
  - alert: etcdHighNumberOfFailedGRPCRequests
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for
        {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
      summary: etcd cluster has high number of failed grpc requests.
    expr: "100 * sum(rate(grpc_server_handled_total{job=~\".*etcd.*\", grpc_code=~\"\
      Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\"\
      }[5m])) without (grpc_type, grpc_code)\n  /\nsum(rate(grpc_server_handled_total{job=~\"\
      .*etcd.*\"}[5m])) without (grpc_type, grpc_code)\n  > 1"
    for: 10m
    labels:
      severity: warning
  - alert: etcdHighNumberOfFailedGRPCRequests
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for
        {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
      summary: etcd cluster has high number of failed grpc requests.
    expr: "100 * sum(rate(grpc_server_handled_total{job=~\".*etcd.*\", grpc_code=~\"\
      Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\"\
      }[5m])) without (grpc_type, grpc_code)\n  /\nsum(rate(grpc_server_handled_total{job=~\"\
      .*etcd.*\"}[5m])) without (grpc_type, grpc_code)\n  > 5"
    for: 5m
    labels:
      severity: critical
  - alert: etcdGRPCRequestsSlow
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": 99th percentile of gRPC requests
        is {{ $value }}s on etcd instance {{ $labels.instance }} for {{ $labels.grpc_method
        }} method.'
      summary: etcd grpc requests are slow
    expr: 'histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*",
      grpc_method!="Defragment", grpc_type="unary"}[5m])) without(grpc_type))

      > 0.15'
    for: 10m
    labels:
      severity: critical
  - alert: etcdMemberCommunicationSlow
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": member communication with {{
        $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance
        }}.'
      summary: etcd cluster member communication is slow.
    expr: 'histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))

      > 0.15'
    for: 10m
    labels:
      severity: warning
  - alert: etcdHighNumberOfFailedProposals
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures
        within the last 30 minutes on etcd instance {{ $labels.instance }}.'
      summary: etcd cluster has high number of proposal failures.
    expr: rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
    for: 15m
    labels:
      severity: warning
  - alert: etcdHighFsyncDurations
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": 99th percentile fsync durations
        are {{ $value }}s on etcd instance {{ $labels.instance }}.'
      summary: etcd cluster 99th percentile fsync durations are too high.
    expr: 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))

      > 0.5'
    for: 10m
    labels:
      severity: warning
  - alert: etcdHighFsyncDurations
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": 99th percentile fsync durations
        are {{ $value }}s on etcd instance {{ $labels.instance }}.'
      summary: etcd cluster 99th percentile fsync durations are too high.
    expr: 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))

      > 1'
    for: 10m
    labels:
      severity: critical
  - alert: etcdHighCommitDurations
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": 99th percentile commit durations
        {{ $value }}s on etcd instance {{ $labels.instance }}.'
      summary: etcd cluster 99th percentile commit durations are too high.
    expr: 'histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))

      > 0.25'
    for: 10m
    labels:
      severity: warning
  - alert: etcdDatabaseQuotaLowSpace
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": database size exceeds the defined
        quota on etcd instance {{ $labels.instance }}, please defrag or increase the
        quota as the writes to etcd will be disabled when it is full.'
      summary: etcd cluster database is running full.
    expr: (last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[5m]) /
      last_over_time(etcd_server_quota_backend_bytes{job=~".*etcd.*"}[5m]))*100 >
      95
    for: 10m
    labels:
      severity: critical
  - alert: etcdExcessiveDatabaseGrowth
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": Predicting running out of disk
        space in the next four hours, based on write observations within the past
        four hours on etcd instance {{ $labels.instance }}, please check as it might
        be disruptive.'
      summary: etcd cluster database growing very fast.
    expr: predict_linear(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[4h], 4*60*60)
      > etcd_server_quota_backend_bytes{job=~".*etcd.*"}
    for: 10m
    labels:
      severity: warning
  - alert: etcdDatabaseHighFragmentationRatio
    annotations:
      description: 'etcd cluster "{{ $labels.job }}": database size in use on instance
        {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual
        allocated disk space, please run defragmentation (e.g. etcdctl defrag) to
        retrieve the unused fragmented disk space.'
      runbook_url: https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation
      summary: etcd database size in use is less than 50% of the actual allocated
        storage.
    expr: (last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes{job=~".*etcd.*"}[5m])
      / last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[5m])) < 0.5
      and etcd_mvcc_db_total_size_in_use_in_bytes{job=~".*etcd.*"} > 104857600
    for: 10m
    labels:
      severity: warning
- name: config-reloaders
  rules:
  - alert: ConfigReloaderSidecarErrors
    annotations:
      description: 'Errors encountered while the {{$labels.pod}} config-reloader sidecar
        attempts to sync config in {{$labels.namespace}} namespace.

        As a result, configuration for service running in {{$labels.pod}} may be stale
        and cannot be updated anymore.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors
      summary: config-reloader sidecar has not had a successful reload for 10m
    expr: max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
    for: 10m
    labels:
      severity: warning
- name: kubernetes-storage
  rules:
  - alert: KubePersistentVolumeErrors
    annotations:
      description: The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster
        -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
      summary: PersistentVolume is having issues with provisioning.
    expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kubernetes-service-endpoints"}
      > 0
    for: 5m
    labels:
      severity: critical
- interval: 3m
  name: kube-apiserver-availability.rules
  rules:
  - expr: avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24 *
      30
    record: code_verb:apiserver_request_total:increase30d
  - expr: sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
    labels:
      verb: read
    record: code:apiserver_request_total:increase30d
  - expr: sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
    labels:
      verb: write
    record: code:apiserver_request_total:increase30d
  - expr: sum by (cluster, verb, scope, le) (increase(apiserver_request_sli_duration_seconds_bucket[1h]))
    record: cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h
  - expr: sum by (cluster, verb, scope, le) (avg_over_time(cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h[30d])
      * 24 * 30)
    record: cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d
  - expr: sum by (cluster, verb, scope) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h{le="+Inf"})
    record: cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase1h
  - expr: sum by (cluster, verb, scope) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le="+Inf"})
    record: cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d
  - expr: "1 - (\n  (\n    # write too slow\n    sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"\
      POST|PUT|PATCH|DELETE\"})\n    -\n    sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      POST|PUT|PATCH|DELETE\",le=~\"1(\\\\.0)?\"} or vector(0))\n  ) +\n  (\n    #\
      \ read too slow\n    sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"\
      LIST|GET\"})\n    -\n    (\n      sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      LIST|GET\",scope=~\"resource|\",le=~\"1(\\\\.0)?\"} or vector(0))\n      +\n\
      \      sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      LIST|GET\",scope=\"namespace\",le=~\"5(\\\\.0)?\"} or vector(0))\n      +\n\
      \      sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      LIST|GET\",scope=\"cluster\",le=~\"30(\\\\.0)?\"} or vector(0))\n    )\n  )\
      \ +\n  # errors\n  sum by (cluster) (code:apiserver_request_total:increase30d{code=~\"\
      5..\"} or vector(0))\n)\n/\nsum by (cluster) (code:apiserver_request_total:increase30d)"
    labels:
      verb: all
    record: apiserver_request:availability30d
  - expr: "1 - (\n  sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"\
      LIST|GET\"})\n  -\n  (\n    # too slow\n    sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      LIST|GET\",scope=~\"resource|\",le=~\"1(\\\\.0)?\"} or vector(0))\n    +\n \
      \   sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      LIST|GET\",scope=\"namespace\",le=~\"5(\\\\.0)?\"} or vector(0))\n    +\n  \
      \  sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      LIST|GET\",scope=\"cluster\",le=~\"30(\\\\.0)?\"} or vector(0))\n  )\n  +\n\
      \  # errors\n  sum by (cluster) (code:apiserver_request_total:increase30d{verb=\"\
      read\",code=~\"5..\"} or vector(0))\n)\n/\nsum by (cluster) (code:apiserver_request_total:increase30d{verb=\"\
      read\"})"
    labels:
      verb: read
    record: apiserver_request:availability30d
  - expr: "1 - (\n  (\n    # too slow\n    sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"\
      POST|PUT|PATCH|DELETE\"})\n    -\n    sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~\"\
      POST|PUT|PATCH|DELETE\",le=~\"1(\\\\.0)?\"} or vector(0))\n  )\n  +\n  # errors\n\
      \  sum by (cluster) (code:apiserver_request_total:increase30d{verb=\"write\"\
      ,code=~\"5..\"} or vector(0))\n)\n/\nsum by (cluster) (code:apiserver_request_total:increase30d{verb=\"\
      write\"})"
    labels:
      verb: write
    record: apiserver_request:availability30d
  - expr: sum by (cluster,code,resource) (rate(apiserver_request_total{job="kubernetes-apiservers",verb=~"LIST|GET"}[5m]))
    labels:
      verb: read
    record: code_resource:apiserver_request_total:rate5m
  - expr: sum by (cluster,code,resource) (rate(apiserver_request_total{job="kubernetes-apiservers",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
    labels:
      verb: write
    record: code_resource:apiserver_request_total:rate5m
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="kubernetes-apiservers",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"2.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="kubernetes-apiservers",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"3.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="kubernetes-apiservers",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"4.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="kubernetes-apiservers",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h
- name: node.rules
  rules:
  - expr: "avg by (cluster) (\n  node:node_cpu_utilization:ratio_rate5m\n)"
    record: cluster:node_cpu:ratio_rate5m
